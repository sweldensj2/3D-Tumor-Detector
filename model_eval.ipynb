{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4949554d-8289-4b80-a50d-87b21719fc1d",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "The goal here is to load in a model and evaluate its performance on a random valid 3d MRI scan. The goal is to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8cdd4ff-adc8-43f1-9b23-3bf9659b0166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name:  Tesla T4\n",
      "Allocated Memory: 0.00 GB\n",
      "Cached Memory: 0.00 GB\n",
      "Total GPU Memory: 14.58 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import nibabel as nib\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "final_project_path = '/home/jws2215/e6691-2024spring-project-jwss-jws2215' # vm\n",
    "data_folder_path = os.path.join(final_project_path, 'BraTS2020')\n",
    "train_folder_path = os.path.join(data_folder_path, 'train')\n",
    "valid_folder_path = os.path.join(data_folder_path, 'valid')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "    allocated_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "    cached_memory = torch.cuda.memory_reserved() / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "    print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "    print(f\"Cached Memory: {cached_memory:.2f} GB\")\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Cannot print memory usage.\")\n",
    "    device = torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e44565-4b06-4f99-a34f-4b4538324e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_data_dictionary(folder_path):\n",
    "    data_dict = {}\n",
    "    subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "    \n",
    "    for idx, subfolder in enumerate(subfolders):\n",
    "        abs_path = os.path.join(folder_path, subfolder)\n",
    "        data_dict[idx] = {'absolute_path': abs_path, 'folder_name': subfolder}\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "data_path_dictionary_train = create_data_dictionary(train_folder_path)\n",
    "data_path_dictionary_valid = create_data_dictionary(valid_folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a5dc056-94bf-40f9-9172-89fff2703603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## all data is stored in (240,240,155) \n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_path_dictionary):\n",
    "        self.data_path_dictionary = data_path_dictionary\n",
    "\n",
    "    def __len__(self):\n",
    "        # print(len(self.annotations[\"images\"]))\n",
    "        return len(self.data_path_dictionary)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        folder_name = self.data_path_dictionary[idx][\"folder_name\"]\n",
    "        folder_path = self.data_path_dictionary[idx][\"absolute_path\"]\n",
    "        \n",
    "        seg_path = os.path.join(folder_path, folder_name + '_seg.nii')\n",
    "        t1_path = os.path.join(folder_path, folder_name + '_t1.nii')\n",
    "        t1ce_path = os.path.join(folder_path, folder_name + '_t1ce.nii')\n",
    "        t2_path = os.path.join(folder_path, folder_name + '_t2.nii')\n",
    "        flair_path = os.path.join(folder_path, folder_name + '_flair.nii')\n",
    "        \n",
    "        # Load .nii files as nparrays\n",
    "        seg_img = nib.load(seg_path).get_fdata()\n",
    "        \n",
    "        t1_img = nib.load(t1_path).get_fdata() #combine these ones\n",
    "        t1ce_img = nib.load(t1ce_path).get_fdata()#combine these ones\n",
    "        t2_img = nib.load(t2_path).get_fdata()#combine these ones\n",
    "        flair_img = nib.load(flair_path).get_fdata()#combine these ones\n",
    "        \n",
    "        # Combine the MRI scans into a single 4-channel image\n",
    "        combined_mri = np.stack([t1_img, t1ce_img, t2_img, flair_img], axis=0)  \n",
    "        \n",
    "        # Convert combined_mri and seg_img to torch tensors\n",
    "        # combined_mri = torch.tensor(combined_mri, dtype=torch.float32)\n",
    "        # seg_img = torch.tensor(seg_img, dtype=torch.float32)\n",
    "        combined_mri = torch.tensor(combined_mri, dtype=torch.int32)\n",
    "        seg_img = torch.tensor(seg_img, dtype=torch.int32)\n",
    "\n",
    "        \n",
    "        #convert to binary problem:\n",
    "        seg_img[seg_img != 0] = 1\n",
    "        \n",
    "        return combined_mri, seg_img\n",
    "\n",
    "val_dataset = ImageDataset(data_path_dictionary_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430e9c6-3ff4-4725-a508-eb68416fdb54",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8340b5-c8db-42c5-8c45-53667ee85b43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model UNetPaper(\n",
      "  (down_conv): ModuleList(\n",
      "    (0): DoubleConvolution(\n",
      "      (first): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act1): ReLU()\n",
      "      (second): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act2): ReLU()\n",
      "    )\n",
      "    (1): DoubleConvolution(\n",
      "      (first): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act1): ReLU()\n",
      "      (second): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act2): ReLU()\n",
      "    )\n",
      "    (2): DoubleConvolution(\n",
      "      (first): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act1): ReLU()\n",
      "      (second): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act2): ReLU()\n",
      "    )\n",
      "    (3): DoubleConvolution(\n",
      "      (first): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act1): ReLU()\n",
      "      (second): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (down_sample): ModuleList(\n",
      "    (0-3): 4 x DownSample(\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (middle_conv): DoubleConvolution(\n",
      "    (first): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (act1): ReLU()\n",
      "    (second): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (act2): ReLU()\n",
      "  )\n",
      "  (up_sample): ModuleList(\n",
      "    (0): UpSample(\n",
      "      (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (1): UpSample(\n",
      "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (2): UpSample(\n",
      "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (3): UpSample(\n",
      "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (up_conv): ModuleList(\n",
      "    (0): DoubleConvolution(\n",
      "      (first): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act1): ReLU()\n",
      "      (second): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act2): ReLU()\n",
      "    )\n",
      "    (1): DoubleConvolution(\n",
      "      (first): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act1): ReLU()\n",
      "      (second): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act2): ReLU()\n",
      "    )\n",
      "    (2): DoubleConvolution(\n",
      "      (first): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act1): ReLU()\n",
      "      (second): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act2): ReLU()\n",
      "    )\n",
      "    (3): DoubleConvolution(\n",
      "      (first): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act1): ReLU()\n",
      "      (second): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (act2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (concat): ModuleList(\n",
      "    (0-3): 4 x CropAndConcat()\n",
      "  )\n",
      "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from seg_models import UNet, UNetPaper\n",
    "    \n",
    "    \n",
    "# Define the number of classes\n",
    "num_output_classes = 1 #normally should be 5\n",
    "num_input_channels = 4\n",
    "\n",
    "# Custom Models\n",
    "# model = UNet(num_input_channels, num_output_classes)\n",
    "\n",
    "# Paper Models\n",
    "model = UNetPaper(num_input_channels, num_output_classes)\n",
    "\n",
    "# Load the saved weights\n",
    "final_project_path \n",
    "saved_models_path = os.path.join(final_project_path, 'saved_models')\n",
    "\n",
    "\n",
    "####### Change the name here #########\n",
    "saved_model_path = os.path.join(saved_models_path, 'train_unet2_bce_lr1e-5_6e')\n",
    "\n",
    "# Add the file name\n",
    "saved_weights_path = 'last_unet.pth'\n",
    "saved_weights_path = os.path.join(saved_model_path, saved_weights_path)\n",
    "saved_weights = torch.load(saved_weights_path)\n",
    "\n",
    "# Load the weights into the model\n",
    "model.load_state_dict(saved_weights)\n",
    "model = model.to(device)\n",
    "print(\"model\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efb70b-8561-46f5-bc31-eb21d0475bbe",
   "metadata": {},
   "source": [
    "# Evaluation on a Valid Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac5119b5-81f3-4a60-ab63-83feb52cf88d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_mri torch.Size([4, 240, 240, 155])\n",
      "seg_img torch.Size([240, 240, 155])\n",
      "Done: 15_results.mp4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import cv2\n",
    "\n",
    "def update(frame, frames):\n",
    "    ax.clear()\n",
    "    ax.imshow(frames[frame])\n",
    "    ax.set_title(f\"Layer {frame}\")\n",
    "    ax.axis('off')\n",
    "    return ax,\n",
    "\n",
    "def overlay_mask_on_flair(flair_img, mask_img):\n",
    "    # Convert flair image to RGB\n",
    "    flair_rgb = np.stack([flair_img, flair_img, flair_img], axis=-1)\n",
    "    \n",
    "    # Normalize mask image\n",
    "    mask_norm = (mask_img - np.min(mask_img)) / (np.max(mask_img) - np.min(mask_img))\n",
    "    \n",
    "    # Create alpha channel for mask\n",
    "    alpha = 0.5  # Adjust transparency here\n",
    "    mask_alpha = np.zeros_like(flair_img)\n",
    "    mask_alpha[mask_img > 0] = alpha\n",
    "    \n",
    "    # Create overlay image\n",
    "    overlay_img = flair_rgb.copy()\n",
    "    overlay_img[:, :, 0] = flair_rgb[:, :, 0] * (1 - mask_norm) + mask_norm * 255\n",
    "    overlay_img[:, :, 1] = flair_rgb[:, :, 1] * (1 - mask_norm)\n",
    "    overlay_img[:, :, 2] = flair_rgb[:, :, 2] * (1 - mask_norm)\n",
    "    \n",
    "    # Apply alpha channel\n",
    "    overlay_img = overlay_img.astype(np.uint8)\n",
    "    overlay_img = np.dstack((overlay_img, mask_alpha))\n",
    "    \n",
    "    return overlay_img\n",
    "\n",
    "def red_mask_on_flair(flair_img, mask_img, display_threshold):\n",
    "    # Convert flair image to RGB\n",
    "    flair_rgb = np.stack([flair_img, flair_img, flair_img], axis=-1)\n",
    "    \n",
    "    \n",
    "    # Normalize mask image\n",
    "    mask_norm = (mask_img - np.min(mask_img)) / (np.max(mask_img) - np.min(mask_img))\n",
    "    \n",
    "    # Create overlay image\n",
    "    red_overlay_img = flair_rgb.copy()\n",
    "    red_overlay_img = red_overlay_img.astype(np.uint8)\n",
    "    red_overlay_img = cv2.cvtColor(red_overlay_img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # threshold applied\n",
    "    sample_output = mask_img.copy()\n",
    "    sample_output[sample_output >= display_threshold] = 1\n",
    "    sample_output[sample_output < display_threshold] = 0\n",
    "    \n",
    "    sample_output = sample_output.astype(np.uint8)\n",
    "    \n",
    "    # apply overlay\n",
    "    # Overlay the predicted mask on top of the original image\n",
    "    red_color = np.array([255, 0, 0], dtype=np.uint8)\n",
    "    red_overlay_img[sample_output == 1] = red_overlay_img[sample_output == 1] * 0.8 + red_color * 0.3  # Mix red with original image\n",
    "\n",
    "    \n",
    "    \n",
    "    return red_overlay_img\n",
    "\n",
    "\n",
    "def patient_eval(patient_number, model, val_dataset, device, display_threshold = 0.99, create_video = False):\n",
    "    model.eval()\n",
    "    combined_mri, seg_img = val_dataset.__getitem__(patient_number)\n",
    "    print(\"combined_mri\", combined_mri.shape)\n",
    "    print(\"seg_img\", seg_img.shape)\n",
    "    \n",
    "    if not create_video:\n",
    "        for layer in range(seg_img.shape[2]):\n",
    "            if(layer % 5 != 0):\n",
    "                continue\n",
    "\n",
    "            # Create a subplot with 1 row and 3 columns\n",
    "            fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "\n",
    "            # Display the flair\n",
    "            axes[0].imshow(combined_mri[0, :, :, layer], cmap='gray')\n",
    "            axes[0].set_title(\"Flair\")\n",
    "            axes[0].axis('off')\n",
    "\n",
    "            # Display the segmentation\n",
    "            axes[1].imshow(seg_img[:, :, layer], cmap='hot')\n",
    "            axes[1].set_title(\"Mask\")\n",
    "            axes[1].axis('off')\n",
    "\n",
    "            # Get the prediction\n",
    "            current_slice = combined_mri[:, :, :, layer].unsqueeze(0).float().to(device)\n",
    "            prediction = model(current_slice).squeeze().cpu().detach().numpy()\n",
    "\n",
    "            # Display the prediction\n",
    "            axes[2].imshow(prediction, cmap='hot')\n",
    "            axes[2].set_title(\"Prediction\")\n",
    "            axes[2].axis('off')\n",
    "\n",
    "            # Overlay the prediction on the flair image\n",
    "            overlay_img = overlay_mask_on_flair(combined_mri[0, :, :, layer], prediction)\n",
    "            overlay_img = overlay_img[:, :, :3]\n",
    "            axes[3].imshow(overlay_img)\n",
    "            axes[3].set_title(\"Prediction Overlay\")\n",
    "            axes[3].axis('off')\n",
    "            \n",
    "            # Overlay the see through prediction on the flair image\n",
    "            red_overlay_img = red_mask_on_flair(combined_mri[0, :, :, layer], prediction, display_threshold)\n",
    "            axes[4].imshow(red_overlay_img)\n",
    "            axes[4].set_title(\"99% Threshold Overlay\")\n",
    "            axes[4].axis('off')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "    if(create_video):\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Change codec as needed\n",
    "        results_path = '/home/jws2215/e6691-2024spring-project-jwss-jws2215/results' # vm\n",
    "        name = str(patient_number) + \"_results.mp4\"\n",
    "        results_path = os.path.join(results_path, name)\n",
    "        fps = 15\n",
    "        frame_width = 480\n",
    "        frame_height = 240\n",
    "        output_video = cv2.VideoWriter(results_path, fourcc, fps, (frame_width, frame_height))\n",
    "        \n",
    "        \n",
    "        numb_passes = 1 # Number of forward-backward passes\n",
    "        layers_order = list(range(seg_img.shape[2])) + list(range(seg_img.shape[2] - 2, 0, -1))\n",
    "        layers_order *= numb_passes  # Repeat the list numb_passes times\n",
    "        \n",
    "        for layer in layers_order:\n",
    "            # Get the prediction\n",
    "            current_slice = combined_mri[:, :, :, layer].unsqueeze(0).float().to(device)\n",
    "            prediction = model(current_slice).squeeze().cpu().detach().numpy()\n",
    "\n",
    "            # Overlay the prediction on the flair image\n",
    "            overlay_img = overlay_mask_on_flair(combined_mri[0, :, :, layer], prediction)\n",
    "            overlay_img = overlay_img[:, :, :3]\n",
    "            overlay_img = overlay_img.astype(np.uint8)\n",
    "            overlay_img = cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Get Red overlay\n",
    "            red_overlay_img = red_mask_on_flair(combined_mri[0, :, :, layer], prediction, display_threshold).astype(np.uint8)\n",
    "            red_overlay_img = cv2.cvtColor(red_overlay_img, cv2.COLOR_RGB2BGR)\n",
    "            # Combine the images\n",
    "            combined_img = np.hstack((overlay_img, red_overlay_img))\n",
    "            \n",
    "            \n",
    "            # Write some Text\n",
    "            font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            bottomLeftCornerOfText = (10,10)\n",
    "            fontScale              = 0.25\n",
    "            fontColor              = (255,255,255)\n",
    "            thickness              = 1\n",
    "            lineType               = 2\n",
    "            \n",
    "            text_to_add = \"Layer: \" + str(layer) + \", Disp Threshold: \" + str(display_threshold) + \", Max Certainity: \" + str(np.max(prediction))\n",
    "            overlay_img = cv2.putText(combined_img,text_to_add, \n",
    "                bottomLeftCornerOfText, \n",
    "                font, \n",
    "                fontScale,\n",
    "                fontColor,\n",
    "                thickness,\n",
    "                lineType)\n",
    "            \n",
    "            output_video.write(combined_img)\n",
    "            \n",
    "        output_video.release()\n",
    "        \n",
    "        print(\"Done:\", name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "patient_number = 15 # range between 1-19\n",
    "display_threshold = 0.95 # 99% threshold to apply.\n",
    "patient_eval(patient_number, model, val_dataset, device, display_threshold = display_threshold, create_video = True)\n",
    "# patient_video(patient_number, model, val_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb9d00-9b9a-4bda-9e92-7e66cf806e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
