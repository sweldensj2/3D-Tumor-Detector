{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cda4673-47fc-427a-a8d6-33e53ea9af2c",
   "metadata": {},
   "source": [
    "# Load Packages and State Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebe1330f-7a58-404e-9b40-93244980f078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name:  Tesla T4\n",
      "Allocated Memory: 0.00 GB\n",
      "Cached Memory: 0.00 GB\n",
      "Total GPU Memory: 14.58 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import nibabel as nib\n",
    "\n",
    "final_project_path = '/home/jws2215/e6691-2024spring-project-jwss-jws2215' # vm\n",
    "data_folder_path = os.path.join(final_project_path, 'BraTS2020')\n",
    "train_folder_path = os.path.join(data_folder_path, 'train')\n",
    "valid_folder_path = os.path.join(data_folder_path, 'valid')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "    allocated_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "    cached_memory = torch.cuda.memory_reserved() / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "    print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "    print(f\"Cached Memory: {cached_memory:.2f} GB\")\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Cannot print memory usage.\")\n",
    "    device = torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc883289-4fd6-4c5d-8d40-f6e0953acec6",
   "metadata": {},
   "source": [
    "# Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee1c97-aa38-4240-b84f-696a4d07922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all images are clean and 640 by 640\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_folder, annotations, image_size=(240, 240), transform=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # print(len(self.annotations[\"images\"]))\n",
    "        return len(self.annotations[\"images\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_list = self.annotations[\"images\"]\n",
    "        # print(\"id\", image_list[idx][\"id\"])\n",
    "        img_name = os.path.join(self.data_folder, image_list[idx]['file_name'])\n",
    "        image = cv2.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Generate mask\n",
    "        mask = self.generate_mask_from_annotations(idx)\n",
    "        # print(\"mask\", mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def generate_mask_from_annotations(self, idx):\n",
    "        # Generate mask from annotations\n",
    "        \n",
    "        mask = np.zeros(self.image_size, dtype=np.uint8)\n",
    "        #mask = np.zeros((*self.image_size, self.num_classes), dtype=np.uint8)\n",
    "        \n",
    "        this_image_annotations = self.annotations[\"annotations\"][idx]\n",
    "        # print(this_image_annotations)\n",
    "        category_id = this_image_annotations[\"category_id\"]\n",
    "        polygons_list = this_image_annotations[\"segmentation\"]\n",
    "        # print(\"category_id\", category_id)\n",
    "        # print(\"polgyons_list\", polygons_list)\n",
    "        \n",
    "        for points in polygons_list:\n",
    "            # Extract annotation points and draw rectangle on the mask\n",
    "            # Last point is the same as the first point\n",
    "            # print(\"points\", points)\n",
    "            \n",
    "            rect_points = np.array(points, dtype=np.int32).reshape((-1, 2))\n",
    "            # print(\"rect_points\", rect_points)\n",
    "            mask = cv2.fillPoly(mask, [rect_points], color=(255))\n",
    "            \n",
    "        if self.transform:\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        #lets do one class only\n",
    "        mask[mask > 0] = 1\n",
    "            \n",
    "        return mask\n",
    "\n",
    "# Define transforms for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((240, 240)),  # Resize to a standard size 240, 240 works fine, 224 from facebook model\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
